"""
å†å²æ•°æ®å›æ”¾å™¨ (Data Replay Agent)
===================================

æ¨¡æ‹Ÿ DataSyncAgentï¼Œä»å†å²æ•°æ®ç”Ÿæˆ MarketSnapshot
ç”¨äºå›æµ‹æ—¶æä¾›ä¸å®ç›˜ç›¸åŒçš„æ•°æ®æ¥å£

Author: AI Trader Team
Date: 2025-12-31
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Iterator, Optional, Tuple
from dataclasses import dataclass, field
import pandas as pd
import numpy as np
import os

from src.api.binance_client import BinanceClient
from src.agents.data_sync_agent import MarketSnapshot
from src.utils.logger import log


@dataclass
class FundingRateRecord:
    """èµ„é‡‘è´¹ç‡è®°å½•"""
    timestamp: datetime
    funding_rate: float
    mark_price: float


@dataclass
class DataCache:
    """å†å²æ•°æ®ç¼“å­˜"""
    symbol: str
    df_5m: pd.DataFrame
    df_15m: pd.DataFrame
    df_1h: pd.DataFrame
    start_date: datetime
    end_date: datetime
    funding_rates: List['FundingRateRecord'] = field(default_factory=list)  # èµ„é‡‘è´¹ç‡å†å²



class DataReplayAgent:
    """
    å†å²æ•°æ®å›æ”¾å™¨
    
    åŠŸèƒ½ï¼š
    1. ä» Binance è·å–å†å² K çº¿æ•°æ®
    2. æœ¬åœ°ç¼“å­˜ï¼ˆParquet æ ¼å¼ï¼‰
    3. åœ¨æŒ‡å®šæ—¶é—´ç‚¹ç”Ÿæˆ MarketSnapshot
    4. æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµç”¨äºå›æµ‹
    """
    
    CACHE_DIR = "data/backtest_cache"
    
    def __init__(
        self,
        symbol: str,
        start_date: str,
        end_date: str,
        client: BinanceClient = None
    ):
        """
        åˆå§‹åŒ–æ•°æ®å›æ”¾å™¨
        
        Args:
            symbol: äº¤æ˜“å¯¹ (e.g., "BTCUSDT")
            start_date: å¼€å§‹æ—¥æœŸ "YYYY-MM-DD"
            end_date: ç»“æŸæ—¥æœŸ "YYYY-MM-DD"
            client: Binance å®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        """
        self.symbol = symbol
        self.start_date = datetime.strptime(start_date, "%Y-%m-%d")
        self.end_date = datetime.strptime(end_date, "%Y-%m-%d")
        self.client = client or BinanceClient()
        
        # æ•°æ®ç¼“å­˜
        self.data_cache: Optional[DataCache] = None
        
        # å½“å‰å›æ”¾ä½ç½®
        self.current_idx = 0
        self.timestamps: List[datetime] = []
        
        # æœ€æ–°å¿«ç…§ï¼ˆæ¨¡æ‹Ÿ DataSyncAgent.latest_snapshotï¼‰
        self.latest_snapshot: Optional[MarketSnapshot] = None
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        os.makedirs(self.CACHE_DIR, exist_ok=True)
        
        log.info(f"ğŸ“¼ DataReplayAgent initialized | {symbol} | {start_date} to {end_date}")
    
    async def load_data(self) -> bool:
        """
        åŠ è½½å†å²æ•°æ®ï¼ˆä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸåŠ è½½
        """
        cache_file = self._get_cache_path()
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if os.path.exists(cache_file):
            log.info(f"ğŸ“‚ Loading cached data from {cache_file}")
            try:
                self._load_from_cache(cache_file)
                log.info(f"âœ… Loaded {len(self.timestamps)} timestamps from cache")
                return True
            except Exception as e:
                log.warning(f"Cache load failed: {e}, fetching from API...")
        
        # ä» API è·å–
        log.info(f"ğŸ“¥ Fetching historical data from Binance API...")
        try:
            await self._fetch_from_api()
            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_to_cache(cache_file)
            log.info(f"âœ… Fetched and cached {len(self.timestamps)} timestamps")
            return True
        except Exception as e:
            log.error(f"âŒ Failed to fetch historical data: {e}")
            return False
    
    def _get_cache_path(self) -> str:
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        start_str = self.start_date.strftime("%Y%m%d")
        end_str = self.end_date.strftime("%Y%m%d")
        return os.path.join(
            self.CACHE_DIR,
            f"{self.symbol}_{start_str}_{end_str}.parquet"
        )
    
    async def _fetch_from_api(self):
        """ä» Binance API è·å–å†å²æ•°æ®"""
        # è®¡ç®—éœ€è¦çš„ K çº¿æ•°é‡
        days = (self.end_date - self.start_date).days + 1
        
        # 5m Kçº¿ï¼šæ¯å¤© 288 æ ¹
        limit_5m = days * 288
        # 15m Kçº¿ï¼šæ¯å¤© 96 æ ¹
        limit_15m = days * 96
        # 1h Kçº¿ï¼šæ¯å¤© 24 æ ¹
        limit_1h = days * 24
        
        # Binance API é™åˆ¶å•æ¬¡æœ€å¤š 1500 æ ¹ï¼Œéœ€è¦åˆ†æ‰¹è·å–
        df_5m = await self._fetch_klines_batched("5m", limit_5m)
        df_15m = await self._fetch_klines_batched("15m", limit_15m)
        df_1h = await self._fetch_klines_batched("1h", limit_1h)
        
        # è·å–èµ„é‡‘è´¹ç‡å†å²
        funding_rates = await self._fetch_funding_rates()
        
        # è¿‡æ»¤æ—¥æœŸèŒƒå›´
        df_5m = self._filter_date_range(df_5m)
        df_15m = self._filter_date_range(df_15m)
        df_1h = self._filter_date_range(df_1h)
        
        # åˆ›å»ºç¼“å­˜å¯¹è±¡
        self.data_cache = DataCache(
            symbol=self.symbol,
            df_5m=df_5m,
            df_15m=df_15m,
            df_1h=df_1h,
            start_date=self.start_date,
            end_date=self.end_date,
            funding_rates=funding_rates
        )
        
        # ç”Ÿæˆæ—¶é—´æˆ³åˆ—è¡¨ï¼ˆåŸºäº 5m Kçº¿ï¼‰
        self.timestamps = df_5m.index.tolist()
        
        log.info(f"   5m: {len(df_5m)} candles")
        log.info(f"   15m: {len(df_15m)} candles")
        log.info(f"   1h: {len(df_1h)} candles")
        log.info(f"   Funding rates: {len(funding_rates)} records")
    
    async def _fetch_funding_rates(self) -> List[FundingRateRecord]:
        """è·å–èµ„é‡‘è´¹ç‡å†å²æ•°æ®"""
        funding_records = []
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            start_ts = int(self.start_date.timestamp() * 1000)
            end_ts = int(self.end_date.timestamp() * 1000)
            
            # Binance API æ¯æ¬¡æœ€å¤šè¿”å› 1000 æ¡
            current_start = start_ts
            
            while current_start < end_ts:
                funding_data = self.client.client.futures_funding_rate(
                    symbol=self.symbol,
                    startTime=current_start,
                    endTime=end_ts,
                    limit=1000
                )
                
                if not funding_data:
                    break
                
                for record in funding_data:
                    fr = FundingRateRecord(
                        timestamp=datetime.fromtimestamp(record['fundingTime'] / 1000),
                        funding_rate=float(record['fundingRate']),
                        mark_price=float(record.get('markPrice', 0))
                    )
                    funding_records.append(fr)
                
                if len(funding_data) < 1000:
                    break
                
                # ä¸‹ä¸€æ‰¹ä»æœ€åä¸€æ¡æ—¶é—´ +1 å¼€å§‹
                current_start = funding_data[-1]['fundingTime'] + 1
                await asyncio.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            log.info(f"ğŸ“Š Fetched {len(funding_records)} funding rate records")
            
        except Exception as e:
            log.warning(f"âš ï¸ Failed to fetch funding rates: {e}")
        
        return funding_records
    
    async def _fetch_klines_batched(self, interval: str, total_limit: int) -> pd.DataFrame:
        """åˆ†æ‰¹è·å– K çº¿æ•°æ®"""
        all_klines = []
        batch_size = 1000  # Binance æ¨èçš„æ‰¹æ¬¡å¤§å°
        
        # è®¡ç®—ç»“æŸæ—¶é—´æˆ³
        end_ts = int(self.end_date.timestamp() * 1000)
        
        remaining = total_limit
        current_end = end_ts
        
        while remaining > 0:
            limit = min(batch_size, remaining)
            
            try:
                klines = self.client.client.futures_klines(
                    symbol=self.symbol,
                    interval=interval,
                    endTime=current_end,
                    limit=limit
                )
                
                if not klines:
                    break
                
                all_klines = klines + all_klines  # å€’åºæ’å…¥
                
                # æ›´æ–°ä¸‹ä¸€æ‰¹çš„ç»“æŸæ—¶é—´ï¼ˆå–æœ€æ—©ä¸€æ ¹çš„å¼€å§‹æ—¶é—´ - 1ï¼‰
                current_end = klines[0][0] - 1
                remaining -= len(klines)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(0.1)
                
            except Exception as e:
                log.warning(f"Batch fetch error: {e}")
                break
        
        # è½¬æ¢ä¸º DataFrame
        return self._klines_to_dataframe(all_klines)
    
    def _klines_to_dataframe(self, klines: List) -> pd.DataFrame:
        """å°† K çº¿åˆ—è¡¨è½¬æ¢ä¸º DataFrame"""
        if not klines:
            return pd.DataFrame()
        
        df = pd.DataFrame(klines, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base',
            'taker_buy_quote', 'ignore'
        ])
        
        # è½¬æ¢æ•°æ®ç±»å‹
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df.set_index('timestamp', inplace=True)
        
        for col in ['open', 'high', 'low', 'close', 'volume', 'quote_volume']:
            df[col] = df[col].astype(float)
        
        df['trades'] = df['trades'].astype(int)
        
        return df[['open', 'high', 'low', 'close', 'volume', 'quote_volume', 'trades']]
    
    def _filter_date_range(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤æ—¥æœŸèŒƒå›´"""
        if df.empty:
            return df
        return df[(df.index >= self.start_date) & (df.index <= self.end_date)]
    
    def _save_to_cache(self, cache_path: str):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        if self.data_cache is None:
            return
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        cache_data = {
            'df_5m': self.data_cache.df_5m,
            'df_15m': self.data_cache.df_15m,
            'df_1h': self.data_cache.df_1h,
            'funding_rates': [
                {'timestamp': fr.timestamp, 'funding_rate': fr.funding_rate, 'mark_price': fr.mark_price}
                for fr in self.data_cache.funding_rates
            ],
        }
        
        # ä½¿ç”¨ pickle ä¿å­˜ï¼ˆæ”¯æŒå¤šä¸ª DataFrameï¼‰
        import pickle
        with open(cache_path, 'wb') as f:
            pickle.dump(cache_data, f)
    
    def _load_from_cache(self, cache_path: str):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        import pickle
        with open(cache_path, 'rb') as f:
            cache_data = pickle.load(f)
        
        # åŠ è½½èµ„é‡‘è´¹ç‡ï¼ˆå…¼å®¹æ—§ç¼“å­˜ï¼‰
        funding_rates = []
        if 'funding_rates' in cache_data:
            for fr_dict in cache_data['funding_rates']:
                funding_rates.append(FundingRateRecord(
                    timestamp=fr_dict['timestamp'],
                    funding_rate=fr_dict['funding_rate'],
                    mark_price=fr_dict.get('mark_price', 0)
                ))
        
        self.data_cache = DataCache(
            symbol=self.symbol,
            df_5m=cache_data['df_5m'],
            df_15m=cache_data['df_15m'],
            df_1h=cache_data['df_1h'],
            start_date=self.start_date,
            end_date=self.end_date,
            funding_rates=funding_rates
        )
        
        self.timestamps = self.data_cache.df_5m.index.tolist()
    
    def get_snapshot_at(self, timestamp: datetime, lookback: int = 300) -> MarketSnapshot:
        """
        è·å–æŒ‡å®šæ—¶é—´ç‚¹çš„å¸‚åœºå¿«ç…§
        
        Args:
            timestamp: ç›®æ ‡æ—¶é—´ç‚¹
            lookback: å›çœ‹çš„ K çº¿æ•°é‡
            
        Returns:
            MarketSnapshot å¯¹è±¡ï¼ˆä¸ DataSyncAgent å…¼å®¹ï¼‰
        """
        if self.data_cache is None:
            raise ValueError("Data not loaded. Call load_data() first.")
        
        # è·å–æˆªæ­¢åˆ° timestamp çš„æ•°æ®
        df_5m = self.data_cache.df_5m[self.data_cache.df_5m.index <= timestamp].tail(lookback)
        df_15m = self.data_cache.df_15m[self.data_cache.df_15m.index <= timestamp].tail(lookback // 3)
        df_1h = self.data_cache.df_1h[self.data_cache.df_1h.index <= timestamp].tail(lookback // 12)
        
        # Stable view: æ’é™¤æœ€åä¸€æ ¹ï¼ˆæœªå®Œæˆï¼‰
        # Live view: æœ€åä¸€æ ¹ï¼ˆä½œä¸º Dictï¼‰
        live_5m_dict = df_5m.iloc[-1].to_dict() if len(df_5m) > 0 else {}
        live_15m_dict = df_15m.iloc[-1].to_dict() if len(df_15m) > 0 else {}
        live_1h_dict = df_1h.iloc[-1].to_dict() if len(df_1h) > 0 else {}
        
        snapshot = MarketSnapshot(
            stable_5m=df_5m.iloc[:-1] if len(df_5m) > 1 else df_5m,
            stable_15m=df_15m.iloc[:-1] if len(df_15m) > 1 else df_15m,
            stable_1h=df_1h.iloc[:-1] if len(df_1h) > 1 else df_1h,
            live_5m=live_5m_dict,
            live_15m=live_15m_dict,
            live_1h=live_1h_dict,
            timestamp=timestamp,
            alignment_ok=True,
            fetch_duration=0.0
        )
        
        self.latest_snapshot = snapshot
        return snapshot
    
    def iterate_timestamps(self, step: int = 1) -> Iterator[datetime]:
        """
        è¿­ä»£æ‰€æœ‰å›æµ‹æ—¶é—´ç‚¹
        
        Args:
            step: æ­¥é•¿ï¼ˆ1 = æ¯ 5 åˆ†é’Ÿï¼Œ3 = æ¯ 15 åˆ†é’Ÿï¼Œ12 = æ¯å°æ—¶ï¼‰
            
        Yields:
            datetime æ—¶é—´ç‚¹
        """
        for i in range(0, len(self.timestamps), step):
            self.current_idx = i
            yield self.timestamps[i]
    
    def get_current_price(self) -> float:
        """
        è·å–å½“å‰ä»·æ ¼
        
        CRITICAL FIX (Cycle 2):
        é˜²æ­¢ Look-ahead Biasï¼š
        è¿”å›å½“å‰ K çº¿çš„ Open ä»·æ ¼ï¼Œè€Œä¸æ˜¯ Close ä»·æ ¼ã€‚
        åœ¨å›æµ‹æ—¶åˆ» Tï¼Œæˆ‘ä»¬åªèƒ½çœ‹åˆ° T æ—¶åˆ»çš„å¼€ç›˜ä»·ï¼Œçœ‹ä¸åˆ° T+5m çš„æ”¶ç›˜ä»·ã€‚
        """
        if self.latest_snapshot is None:
            return 0.0
        
        live = self.latest_snapshot.live_5m
        if isinstance(live, dict):
            # ä½¿ç”¨ OPEN ä»·æ ¼
            return float(live.get('open', 0.0))
        elif hasattr(live, 'empty') and not live.empty:
            # ä½¿ç”¨ OPEN ä»·æ ¼
            return float(live['open'].iloc[-1])
        return 0.0
    
    def get_open_price(self) -> float:
        """
        è·å–å½“å‰ K çº¿çš„å¼€ç›˜ä»·
        
        ç”¨äºé˜²æ­¢ Look-ahead Biasï¼š
        - ä¿¡å·è®¡ç®—ä½¿ç”¨ bar[i-1] çš„æ•°æ®
        - äº¤æ˜“æ‰§è¡Œä½¿ç”¨ bar[i] çš„å¼€ç›˜ä»·
        """
        if self.latest_snapshot is None:
            return 0.0
        
        live = self.latest_snapshot.live_5m
        if isinstance(live, dict):
            return float(live.get('open', 0.0))
        elif hasattr(live, 'empty') and not live.empty:
            return float(live['open'].iloc[-1])
        return 0.0
    
    def get_previous_close_price(self) -> float:
        """
        è·å–ä¸Šä¸€æ ¹ K çº¿çš„æ”¶ç›˜ä»·
        
        ç”¨äº Look-ahead Bias é˜²æŠ¤çš„ä¿¡å·è®¡ç®—
        """
        if self.latest_snapshot is None:
            return 0.0
        
        stable = self.latest_snapshot.stable_5m
        if hasattr(stable, 'empty') and not stable.empty:
            return float(stable['close'].iloc[-1])
        return self.get_open_price()
    
    def get_progress(self) -> Tuple[int, int, float]:
        """è·å–å›æ”¾è¿›åº¦"""
        total = len(self.timestamps)
        current = self.current_idx
        pct = (current / total * 100) if total > 0 else 0
        return current, total, pct
    
    def get_funding_rate_at(self, timestamp: datetime) -> Optional[FundingRateRecord]:
        """
        è·å–æŒ‡å®šæ—¶é—´ç‚¹æˆ–ä¹‹å‰æœ€è¿‘çš„èµ„é‡‘è´¹ç‡
        
        Binance èµ„é‡‘è´¹ç‡æ¯ 8 å°æ—¶ç»“ç®—ï¼ˆUTC 00:00, 08:00, 16:00ï¼‰
        """
        if self.data_cache is None or not self.data_cache.funding_rates:
            return None
        
        # æ‰¾åˆ°æ—¶é—´æˆ³ä¹‹å‰æœ€è¿‘çš„èµ„é‡‘è´¹ç‡
        latest_fr = None
        for fr in self.data_cache.funding_rates:
            if fr.timestamp <= timestamp:
                latest_fr = fr
            else:
                break
        
        return latest_fr
    
    def is_funding_settlement_time(self, timestamp: datetime) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ˜¯èµ„é‡‘è´¹ç‡ç»“ç®—æ—¶é—´
        
        Binance åˆçº¦èµ„é‡‘è´¹ç‡ç»“ç®—æ—¶é—´ï¼šUTC 00:00, 08:00, 16:00
        """
        utc_hour = (timestamp.hour - 8) % 24  # å‡è®¾æœ¬åœ°æ—¶åŒºä¸º UTC+8
        utc_minute = timestamp.minute
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç»“ç®—æ—¶åˆ»ï¼ˆå…è®¸å‡ åˆ†é’Ÿè¯¯å·®ï¼‰
        if utc_hour in [0, 8, 16] and utc_minute < 10:
            return True
        return False
    
    def get_funding_rate_for_settlement(self, timestamp: datetime) -> Optional[float]:
        """
        è·å–ç»“ç®—æ—¶åˆ»é€‚ç”¨çš„èµ„é‡‘è´¹ç‡ï¼ˆä»…åœ¨ç»“ç®—æ—¶åˆ»è¿”å›ï¼Œå¦åˆ™è¿”å›Noneï¼‰
        """
        if not self.is_funding_settlement_time(timestamp):
            return None
        
        fr = self.get_funding_rate_at(timestamp)
        if fr and abs((fr.timestamp - timestamp).total_seconds()) < 600:  # 10åˆ†é’Ÿå†…
            return fr.funding_rate
        return None
    
    # ========== DataSyncAgent å…¼å®¹æ¥å£ ==========
    
    async def fetch_all_timeframes(self, symbol: str = None, limit: int = 300) -> MarketSnapshot:
        """
        å…¼å®¹ DataSyncAgent.fetch_all_timeframes æ¥å£
        
        åœ¨å›æµ‹æ¨¡å¼ä¸‹ï¼Œè¿”å›å½“å‰æ—¶é—´ç‚¹çš„å¿«ç…§
        """
        if self.current_idx < len(self.timestamps):
            timestamp = self.timestamps[self.current_idx]
            return self.get_snapshot_at(timestamp, lookback=limit)
        else:
            raise IndexError("Replay finished, no more data")
    
    def get_live_price(self, timeframe: str = '5m') -> float:
        """å…¼å®¹ DataSyncAgent.get_live_price æ¥å£"""
        return self.get_current_price()
    
    def get_stable_dataframe(self, timeframe: str = '5m') -> pd.DataFrame:
        """å…¼å®¹ DataSyncAgent.get_stable_dataframe æ¥å£"""
        if self.latest_snapshot is None:
            return pd.DataFrame()
        
        if timeframe == '5m':
            return self.latest_snapshot.stable_5m
        elif timeframe == '15m':
            return self.latest_snapshot.stable_15m
        elif timeframe == '1h':
            return self.latest_snapshot.stable_1h
        else:
            return self.latest_snapshot.stable_5m


# æµ‹è¯•å‡½æ•°
async def test_data_replay():
    """æµ‹è¯•æ•°æ®å›æ”¾å™¨"""
    print("\n" + "=" * 60)
    print("ğŸ§ª Testing DataReplayAgent")
    print("=" * 60)
    
    # åˆ›å»ºå›æ”¾å™¨ï¼ˆæµ‹è¯• 7 å¤©æ•°æ®ï¼‰
    replay = DataReplayAgent(
        symbol="BTCUSDT",
        start_date="2024-12-01",
        end_date="2024-12-07"
    )
    
    # åŠ è½½æ•°æ®
    success = await replay.load_data()
    print(f"\nâœ… Data loaded: {success}")
    
    if success:
        # è¿­ä»£å‰ 5 ä¸ªæ—¶é—´ç‚¹
        print("\nğŸ“Š First 5 timestamps:")
        for i, ts in enumerate(replay.iterate_timestamps()):
            if i >= 5:
                break
            snapshot = replay.get_snapshot_at(ts)
            price = replay.get_current_price()
            print(f"   {i+1}. {ts} | Price: ${price:.2f}")
        
        # æ˜¾ç¤ºè¿›åº¦
        current, total, pct = replay.get_progress()
        print(f"\nğŸ“ˆ Progress: {current}/{total} ({pct:.1f}%)")
    
    print("\nâœ… DataReplayAgent test complete!")


if __name__ == "__main__":
    asyncio.run(test_data_replay())
